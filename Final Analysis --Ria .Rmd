---
title: "final analysis"
author: "Ria Pan"
date: "May 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
After our exploratory anlaysis, we got a sense of how correlated one feature is to the other and created some visualisations. Based on that result, we are going to build a predictive model on emplyee's attrition. To start off, we will first construct some feature engineering. 

```{r}
library(tidyverse)
data <- read.csv('/Users/riapan/Desktop/final_project_390/HR-Employee-Attrition.csv')

```

## Feature engineering & Categorical Encoding
Noticing that StandardHours, Over18, EmployeeCount has only 1 demension. We deleted these redundant features. 
```{r}
data<- data[-27]
data <- data[-22]
data <- data[-9]
```


Having identified which features contain categorical data in our dataset, we encoded those categorical variables into numerical values for further exploration. We used model.matrix method to creates encoded dummy variables from the categorical variables.
```{r}
dummy <- model.matrix(Attrition ~ ., data)
new <- data.frame(dummy) # change back to a datafram 
new <- new[,-1] # get rid first useless columne
Attrition <- data$Attrition
dataDummy <- cbind(Attrition,new) #add Attrition back 
dataDummy

```


However after quick inspection of the counts of the number of 'Yes' and 'No' in the target variable(Attrition), we found that there is quite a large skew in target as shown
```{r}
#check to find that the data is skewed 
ggplot(dataDummy) + geom_histogram(aes (x = Attrition),stat = "count")
prop.table(table(dataDummy$Attrition))
table(dataDummy$Attrition)
```
In this case we have to keep in mind that there is quite a big imbalance in our target variable. And with imbalanced data sets, an algorithm doesnâ€™t get the necessary information about the minority class to make an accurate prediction. In order to solve imbalanced data, we have considered many techniques such as repeated oversampling and undersampling. And we decided to use an oversampling technique known as SMOTE to treat this imbalance, which will be performed in next section. 


##Implementing Models
Having performed some exploratory data analysis and simple feature engineering as well as having ensured that all categorical values are encoded, we are now ready to proceed onto building our models.

#Splitting Data into Train and Test sets
Before we start training a model, we partitioned our dataset into a training set and a test set with proportion 8:2.
```{r}
#split data into train and test 
library(caret)
set.seed(1234)
splitIndex <- createDataPartition(dataDummy$Attrition, p =0.8, list =FALSE, times =1)
test <- dataDummy[-splitIndex,]
train <- dataDummy[splitIndex,]
```

#SMOTE to oversample due to the skewness in target
Since we have already noted the severe imbalance in the values within the target variable, we used SMOTE function to created some arbirary data in minor class to make the whole categorical distribution in target variable balanced. 
The SMOTE function oversamples your rare event by using bootstrapping and k-nearest neighbor to synthetically create additional observations of that event. 
```{r}
#oversample train data to balance 
library(DMwR)
trainBalance <- SMOTE(Attrition ~ ., train, perc.over = 400, perc.under=131)
table(trainBalance$Attrition)
prop.table(table(trainBalance$Attrition))
```
We create some data in train set and now the proportion of "NO" "Yes" in target variable changed from 0.84:0.16 to 0.51:0.49. We are ready to build model based on this balanced data now. 

#logistic Regression 
```{r}
#NEED IMPLEMENT 
```
We found LR doesn't work well( show accuracy), so we switch to RF blah blah 

##Random Forest Classifier 
The Random Forest method, first introduced by Breiman in 2001 can be grouped under the category of ensemble models. Why ensemble? The building block of a Random Forest is the ubiquitous Decision Tree. The decision tree as a standalone model is often considered a "weak learner" as its predictive performance is relatively poor. However a Random Forest gathers a group (or ensemble) of decision trees and uses their combined predictive capabilities to obtain relatively strong predictive performance - "strong learner".
This principle of using a collection of "weak learners" to come together to create a "strong learner" underpins the basis of ensemble methods which one regularly comes across in Machine learning. 

train model with defaul setting 
```{r}

library(caret)
library(pROC)
set.seed(1234) #set seed to reproducible 
ctrl <- trainControl(method = "cv", number = 5)
rf_model <- train(Attrition~., data = trainBalance, method = "rf", trControl =ctrl)
print(rf_model)
plot(rf_model)


importance <- varImp(rf_model, scale=FALSE)
importance
#rf_model$importance
predictors <- names(trainBalance)[names(trainBalance) != 'Attrition']
pred_rf <- predict(rf_model$finalModel, test[,predictors],type = "prob")
pred_rf
auc <- roc(test$Attrition, pred_rf[,1])
print(auc)
plot(auc, ylim=c(0,1), print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)

```

##ignore this part! run too long !! 
```{r}
#grid search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(1234)
#since the ideal mtry is between sqrtq to 2/q.
tunegrid <- expand.grid(.mtry=c(1:23))
#tunegrid <- expand.grid(.mtry=8)
rf_gridsearch <- train(Attrition~., data=trainBalance, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)

library(pROC)
predictors <- names(trainBalance)[names(trainBalance) != 'Attrition']
pred_rf_gridsearch <- predict(rf_gridsearch$finalModel, test[,predictors],type = "prob")
auc <- roc(test$Attrition, pred_rf_gridsearch[,1])
print(auc)
plot(auc, ylim=c(0,1), print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
```


##feature selection 
```{r}
#since the best mtry = 3
importance_grid <- varImp(rf_gridsearch,scale = FALSE)
print(importance_grid)
plot(importance_grid)

```
We're suprised that marital status are not in the first 20 importance. It may because marital status has high correlation with other features(such as Age). We can also reflect back to the correlation analysis we made in Data Exploratory Analysis 


##building model based on selected data 
```{r}
slData <- data[, c("OverTime", "StockOptionLevel","TotalWorkingYears",
"MonthlyIncome","JobInvolvement","Age","EnvironmentSatisfaction", "JobSatisfaction",
"YearsWithCurrManager","YearsAtCompany","JobLevel","YearsInCurrentRole",
"DailyRate","EmployeeNumber","TrainingTimesLastYear","DistanceFromHome","Education",
"NumCompaniesWorked","HourlyRate","WorkLifeBalance")]
```
