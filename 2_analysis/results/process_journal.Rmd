---
title: "Process Journal"
author: "Ria Pan, James Barbour, Fahmi Khalid"
date: "May 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(PerformanceAnalytics)
library(Hmisc)
library(corrplot)
library(ggthemes)
library(pROC)
library(caret)
library(DMwR)
library(randomForest)
library(e1071)
```

#Abstract

The purpose of this project is to analyze data provided from IBM’s Human resource department in order to understand the common characteristics in employee attrition, or possible employee discrimination. In order to weight the importance of each variable presented, we filtered the data based on variables that were integer type and performed a correlation analysis. By doing so we were able to develop a basic understanding of which variables to include in our predictive model. The predictive model that we have developed has proven to predict employee attrition with an 84% accuracy rate. We also wanted to consider relationships in non-integer type variables in hopes of observing possible discrimination trends that can be supported by the dataset. We developed a number of visuals in order to justify our conclusions. Lastly, we strived to develop a user-friendly Shiny app for possible managers to use in order to predict employee attrition. All code and details shall be presented in the appropriate Rmd files as part of the final project. The final app can be viewed by running the app in the 'dashboard folder' or can be viewed at http://390.jebarb.com.

#Overview and Motivation: 

The issue of keeping one's employees happy and satisfied is a perennial and age-old challenge. If an employee you have invested so much time and money leaves for "greener pastures", then this would mean that you would have to spend even more time and money to hire somebody else. Alongside these reason maintaining happy employees have resulted in increase profit for companies based on various studies done in industry. By understanding the main reasons why employees leave and possible discrimination, the increase probability your company will succeed. 


#Related Work

In class we had discussed effective techniques of communication. As a team we took that to heart and wanted to develop models that were not only easy to understand but visually pleasing. We have looked into various psychology journals, which study employee habits and utility in order to develop a healthy reasoning of what possible variables are important based on published studies.

#Initial Questions
*	Does Marital Status have a significant effect on wages?
*	How does performance rating contribute to monthly income? 
*	How do different departments contribute to employee attrition?
*	How do different job roles contribute to employee attrition?
*	Can we give a general idea of working atmosphere based on department or job role attrition?
*	Can we predict employee attrition?
*	Does a higher education level contribute to lower attrition levels?
*	How can we tell if an employee is happy?
*	Does work life balance concur with years in position or years with company? How about with monthly income?

Some of these questions were answered by using visuals such as scatter plots, bar graphs, and others. While with other questions such as predicting employee attrition, we developed a healthy predictive model based on the random forest classifier, which enabled an 84% accuracy rate. Though some of the questions were straightforward and were answered with yes or no, a few of them were impossible to determine. One impossible question to answer was “How can we tell if an employee is happy.” Essentially this question boils down to looking into background studies, which observe employee utility from psychology journals. Yet with what these studies propose we did not have the data for thus forcing us to abandon a few questions. One of the main issues with the data set was not the organization but the type of the variables and transforming different types to allow for smooth analysis. For example; on two of our team members laptop the “Age” variable operated as a normal numeric yet with the last team mate the “Age” variable only produced errors. Running into small technical issues which were resolved using Google while also performing other yak shaving activities did slow us down but in the end allowed for worthwhile analysis.

```{r}
employee <- read_csv("IBM-HR-Employee-Attrition.csv")
quit <- filter(employee, Attrition=="Yes") #Filter employee data based on those who have quit
stayed <- filter(employee, Attrition=="No") #Filter employee data based on those who have stayed
```

```{r}
# plot1
gender_influence <- employee %>% group_by(Gender, JobRole) %>%  summarise(average_monthly_income=mean(MonthlyIncome))
gender_influence_monthlyincome <- gender_influence %>% mutate(round_income=round(average_monthly_income))
ggplot(gender_influence_monthlyincome, aes(x=Gender,y=average_monthly_income, color=Gender))+
  geom_bar(stat = "identity",width=0.5,position=position_dodge())+
  geom_text(aes(label=round_income), vjust=1.6, color="white",
            position = position_dodge(0.9), size=3.5)+
  facet_wrap(~JobRole)+
  theme_minimal()+
  scale_fill_manual(values=c("red", "blue"))+
  ggtitle("Monthly incomes across Job Roles")+
  scale_y_continuous(name="Average Monthly Income")

#Grouped employee data by "Gender" and "JobRole" in order to summarise average monthly income based on hose to factors. To make the plot look visually pleasing we rounded the Monthly income and placed it inside of the bar graph, we also highlighted which Gender is represented using blue for male and red for female. We removed the theme to increase signal to noise ratio. Finally we made sure to label the graph, x and y axis for clear interpretation.
```

Represents the average monthly incomes based on gender which we then faceted against Job roles to observe the possible pay gap between genders across various Job roles. From first looking at the graph you may see slight gaps between the genders in certain roles such as Research Director and Manager which one might conclude as insignificant. But if we take the sum for the year of monthly income for Research Director then males will be making a total of $199,896 versus females making a total of $181,728. A $18,168 pay gap! 

```{r}
# plot2
Job_roles <- unique(quit$JobRole)
roles_quit <- str_count(Job_roles)
loss_by_job <- data_frame(Job=Job_roles, "Percentage loss"=(roles_quit/1470)*100)
loss_by_job <- loss_by_job %>% mutate(round_loss=round(loss_by_job$`Percentage loss`,digits=2))
loss_by_job <- arrange(loss_by_job, desc(round_loss))
ggplot(loss_by_job, aes(x=reorder(loss_by_job$Job,-loss_by_job$round_loss),y=loss_by_job$round_loss))+
  geom_bar(stat = "identity",width=0.5, position=position_dodge())+
  geom_text(aes(label=loss_by_job$round_loss), vjust=1.6, color="white",
            position = position_dodge(0.9), size=3.5)+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))+
  labs(x="Job Roles",y="Percentage of Loss")+
  ggtitle("Loss percentage across different Job Roles")
#Found all job roles that had quit and counted each position. Then divided the total count of each position by total observation. Arranged the loss by decreasing amount and graphed using bar plot using similar labels as exploratory plot 1.
```

We wanted to know based on Job roles where was the attrition coming from? With the bar graph we found that the top three job roles with the highest attrition by percentage were Healthcare Representative, Manufacturing Director, and Laboratory Technician. So why does this graph matter? We suggest that IBM looks into the underlying reasons why these roles have the highest loss. From the definition we understand that any loss will result in cost for the company. Are employees unhappy in these roles, are they treated fairly, and are there opportunities for growth? 

```{r}
# plot3
testing <- employee %>% group_by(MaritalStatus) %>% summarise(mean=round(mean(MonthlyIncome),digits=0))
ggplot(testing, aes(x=testing$MaritalStatus,y=testing$mean))+
  geom_bar(stat = "identity",width=0.5, position=position_dodge())+
  geom_text(aes(label=testing$mean), vjust=1.6, color="white",
            position = position_dodge(0.9), size=3.5)+
  labs(x="Marital Status",y="Average Monthly Income")+
  ggtitle("Put a ring on it")

#Group the original data set by Marital status and summarize mean monthly income. Then graphed the monthly income averages according to each category of marital status.
```

After grouping by relationship status, we averaged the monthly income in order to observe any possible trends that occur based on marital status. We concluded that employees who were single observed a deficit of $897 in average monthly income relative to the divorced and married categories.

```{r}
# plot4
RelSat <-employee %>% group_by(RelationshipSatisfaction) %>% summarise(mean=mean(MonthlyIncome))
ggplot(RelSat, aes(x=reorder(RelSat$RelationshipSatisfaction,-RelSat$mean),y=RelSat$mean))+
  geom_bar(stat = "identity",width=0.5, position=position_dodge())+
  geom_text(aes(label=round(RelSat$mean)), vjust=1.6, color="white",
            position = position_dodge(0.9), size=3.5)+
  labs(x="Relationship Satisfaction",y="Average Monthly Income")+
  ggtitle("99 Problems")

#Grouped the original data set based on Relationship Satisfaction and summarized average monthly income. Present the average monthly incomes based on Relationship Satisfaction using a bar graph and used appropriate labels.

```

Similar to the first plot we grouped our x-axis by “Relationship Satisfaction” and observed average monthly income of the 4 groups. We observed that as the level of satisfaction an employee perceives in their relationship the higher average monthly income they earn. Some psychology studies have concluded that a happy and successful relationship outside of work results in being more productive and thus earning more at work. Although we have the background theory, we do not have further data to support these claims, but to simply make observations and state our hypothesis.

```{r}
# plot5
JobSat <- employee %>% group_by( JobSatisfaction) %>% summarise(mean=mean(MonthlyIncome))
Jobsatty <- c("Low", "Medium", "High", "Very High")
ggplot(JobSat, aes(x=reorder(Jobsatty,-JobSat$mean),y=JobSat$mean))+
  geom_bar(stat = "identity",width=0.5, position=position_dodge())+
  geom_text(aes(label=round(JobSat$mean)), vjust=1.6, color="white",
            position = position_dodge(0.9), size=3.5)+
  labs(x="Job Satisfaction",y="Average Monthly Income")+
  ggtitle("Mo Money Mo Problems")

#Grouped the original data set based on Job Satisfaction and summarized average monthly income. Present the average monthly incomes based on Job Satisfaction using a bar graph and used appropriate labels.
```

We observed the average monthly income based on Job satisfaction and found an interesting trend. As the level of satisfaction for one’s job decreases, the average monthly income that one earns increases. Our reasoning begins with the idea that with increased pay comes increased amounts of responsibility, which in turn causes more stress to perform well and ensure that one’s team is performing well. 

```{r}
# plot6
lin_reg <- lm(WorkLifeBalance~ YearsSinceLastPromotion, employee)
summary(lin_reg)

#explored linear regression of work life balance against years since last promotion and summarized.
```

This is a summary of a linear regression of Work life balance against Years since last promotion in an attempt to distinguish the overall atmosphere and attitude an employee who stays longer with the company develops. The results were inconclusive.

```{r}
# plot7
boxplot(MonthlyIncome~WorkLifeBalance,data = employee,xlab="Work Life Balance", ylab="Monthly Income")

#Created a boxplot using monthly income as the y-axis and Work life balance as x-asxis. Then used appropriate labels 
```

We wondered whether Performance rating would have an effect on Monthly income thus we used a box plot to compare the difference performance ratings beside each other. The conclusion was that there was not enough significance between the rating groups to emphasize any trend.

```{r}
# plot8
numeric <- employee[, c(1,4,6,7,10)]
#see correlation between certain features 
res2 <- rcorr(as.matrix(numeric)) # data must be a matrix 

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

flattenCorrMatrix(res2$r, res2$P)
```

```{r}
# plot9
suppressWarnings(chart.Correlation(numeric, histogram=TRUE, pch=19))
```

Performing a correlation against Age, daily rate, distance from home, education, and employee number we did not find any significant correlation. Thus later on we decided to perform a correlation on all numeric variables against each other. 

```{r}
# plot10
genderIncome <- employee %>% group_by(Gender) %>%
  summarise(meanSalary = mean(MonthlyIncome))
ggplot(genderIncome, aes (x = Gender, y = meanSalary)) +  ggtitle("Mean monthly income between gender ")+ geom_bar(stat = "identity",width=0.5, position=position_dodge())+
  geom_text(aes(label=round(meanSalary)), vjust=1.6, color="white",
            position = position_dodge(0.9), size=3.5)
#Grouped the original data set by gender and summarized average monthly salary. This bar graph represents the average monthly salary general between genders. Used similar labels as exploratory plot 1.
```

As a general plot we wanted to understand, regardless of job roles, education, or other factors how does the average monthly income differ between males and females. The surprising conclusion from the graph was that females overall made $306 more money than males. It would be then be helpful to look at our first exploratory plot to understand where this trend may be coming from.

```{r}
# plot11
ggplot(employee, aes(x = Gender, color = Gender)) +
  geom_bar()+ facet_wrap(~ JobRole)+ggtitle("Job distribution among gender")+ ggtitle("Job distribution across gender ")

#Decided to develop a visual of the amount of employees based on gender across all departments. Using geom_bar we were able to find interesting distributions.

```

Understanding how many females and males account for job roles is also an important question to ask. We may move away from monthly salary and ask based on the amount of each gender is there a tendency to hire or attract a certain gender into certain positions. Questions to ask for instance would be; in the Laboratory technician positions is the gap there due to a shortage of qualified female laboratory technicians or are the hiring managers discriminating against females. Now statistics with inadequate data will not give you the immediate answer but it will provide a compass of where to look.

```{r}
# plot12
genderJobIncome <- employee  %>% group_by(Gender, JobRole) %>%
  summarise(meanSalarybyJob = mean(MonthlyIncome))
ggplot(genderJobIncome, aes(x = JobRole,y = meanSalarybyJob, color = Gender)) +
  geom_point() +ggtitle("Mean monthly income among diffrent jobs across gender ")
#By group gender and Job role we then summarized average monthly income and represented it in a scatter plot. Using appropriate labels as exploratory plot 1.
```

By grouping according to gender and Job role we then summarized the mean monthly income in order to create a scatterplot that in theory would emphasize the difference between genders across all job roles. Though a scatterplot in this situation we found not to be the most influential of graphs.

```{r}
# plot13
employee$Attrition[employee$Attrition=="Yes"] <- 1
employee$Attrition[employee$Attrition=="No"] <- 0
integer_data <-employee[,c(1,2,4,6,7,10,11,13,14,15,17,19,20,21,24,25,26,28:35)]
integer_data$Attrition <-as.numeric(integer_data$Attrition)
correlation_data <- cor(integer_data)
corrplot(correlation_data,method="circle")

#First replacing the "Yes" and "No" reponses in the original data set under the attrition column to numeric type allowed us to use it with ease in certain facets wraps. By creating a new data set from only the integer based types of the original variables we were able to develop a correlation between all integer variables.
```

We could have coded a correlation for each variable we may have thought was important individually but this would have taken too much labor. Thus what we ended up doing was segmenting the original data set and by taking only the integer based variables and performing a correlation analysis. The deeper the blue circle, the higher the correlation between the two variables is. This gave us a more specific set of variables to test against one another. The variables we found to be over .70 correlated are:

*	Job Level VS Monthly income
*	Total working years VS Job level
*	Total working years VS Monthly income
*	Total working years VS Age
*	Performance salary hike VS Performance Rating
*	Years at Company VS Years with Current Manager
*	Years at Company VS Years in Current Role
*	Years in Current Role VS Years with Current Manager


```{r}
# plot14
quit <- filter(employee, Attrition==1)
ggplot(data=quit)+
  geom_point(aes(y=quit$MonthlyIncome, x=quit$BusinessTravel,color=JobRole))+
  labs(y="Monthly Income", x="Travels")+
  ggtitle("Attritions trend Income VS Travel")
#By segmenting the original data set into only those who have quit, we then used a scatter plot to examine job roles based on travel frequency.
```

By filtering the employees who have quit, we then graphed monthly average income by amount traveled by employee and observed that the largest group who quit were those who “Rarely traveled.” Now what does this mean? Inconclusive.

```{r}
# plot15
ggplot(data=employee)+
  geom_point(aes(y=employee$MonthlyIncome, x=employee$TotalWorkingYears,color=JobRole))+
  labs(y="Monthly Income", x="Working Years")+
  ggtitle("Observing trend of Income VS Working Years")
# Created a scatter plot that used Monthly income as the y-axis and x-axis as working years faceted against job roles.
```

We placed working years as the x-axis and monthly income as the y-axis to develop a visual of where certain positions lie in terms of working year experience. As expected the more working years an employee may have the higher the monthly income. Also with increased working years results in higher positions such as Manager or Director.

```{r}
# plot16
ggplot(data=employee)+
  geom_point(aes(y=employee$PercentSalaryHike, x=employee$PerformanceRating))+
  labs(y="Percentage Hike", x="Performance Rating")+
  ggtitle("Attritions trend Percentage Hike VS Performance Rating")+
  facet_wrap(~Attrition)

# Created scatter plot with Percent salary hike as y-axis and Performance rating as x-axis.
```

Using the entire dataset we set Performance rating as the independent variable and Performance salary hike as the dependent variable. What we observed was that Performance rating only resulted in a rating of 3 or 4, which made an interesting find as a trend of the company but at the same time did not provide much to conclude on.

#Results
##Some results we have found
* In IBM, average monthly salary of female(6686.566) is higher than that of male (6380.508), which is out of what we expected
* Across all job roles, the different pay between women and men are marginal
* On higher position such as manager or manufacturing director, number of female and male are pretty much equal. But positions of sales executive and research scientist have attracted more male employees than females
* As for marital status, people married or divorced have higher income than people are single, which is interesting to see. It’s also reasonable because they may have longer working years and higher position level. In this case, we plan to explore if job position concur with marital status
* The performance rating in the dataset is either 3 or 4 (good or excellent), maybe everyone at IBM is good at their work
* There is no strong correlation between work life balance and years in company or years in position. Also monthly income has trivial influence on work life balance
* There is positive linear relationship between years at company and years since last promotion, with a significant adjusted R^2 0.382

##Next steps
* Continue running on feature selection to delete irrelevant ones
* Continue on model selection and compare the accuracy between different models
* Perform logistic regression on employee attrition
* Once we finish all analysis, we will put everything in shiny app to form a complete project

#Final Analysis

## Introduction
After our exploratory anlaysis, we got a sense of how correlated one feature is to the other and created some visualisations. Based on that result, we are going to build a predictive model on emplyee's attrition. To start off, we will first construct some feature engineering. 

```{r}
#can't do read_csv here, need be "dataframe" 
data <- read.csv('IBM-HR-Employee-Attrition.csv')
```

## Feature engineering & Categorical Encoding
Noticing that StandardHours, Over18, EmployeeCount has only 1 demension. We deleted these redundant features. 

```{r}
data<- data[-27]
data <- data[-22]
data <- data[-9]
```

Having identified which features contain categorical data in our dataset, we encoded those categorical variables into numerical values for further exploration. We used model.matrix method to creates encoded dummy variables from the categorical variables.

```{r}
dummy <- model.matrix(Attrition ~ ., data)
new <- data.frame(dummy) # change back to a datafram 
new <- new[,-1] # get rid first useless columne
Attrition <- data$Attrition
dataDummy <- cbind(Attrition,new) #add Attrition back 
head(dataDummy[,3:4])
```

However after quick inspection of the counts of the number of 'Yes' and 'No' in the target variable(Attrition), we found that there is quite a large skew in target as shown

```{r}
#check to find that the data is skewed 
ggplot(dataDummy) + geom_histogram(aes (x = Attrition),stat = "count")
prop.table(table(dataDummy$Attrition))
table(dataDummy$Attrition)
```

In this case we have to keep in mind that there is quite a big imbalance in our target variable. And with imbalanced data sets, an algorithm doesn’t get the necessary information about the minority class to make an accurate prediction. In order to solve imbalanced data, we have considered many techniques such as repeated oversampling and undersampling. And we decided to use an oversampling technique known as SMOTE to treat this imbalance, which will be performed in next section. 


##Implementing Models
Having performed some exploratory data analysis and simple feature engineering as well as having ensured that all categorical values are encoded, we are now ready to proceed onto building our models.

###Splitting Data into Train and Test sets
Before we start training a model, we partitioned our dataset into a training set and a test set with proportion 8:2.

```{r}
#split data into train and test 
set.seed(1234)
splitIndex <- createDataPartition(dataDummy$Attrition, p =0.8, list =FALSE, times =1)
test <- dataDummy[-splitIndex,]
train <- dataDummy[splitIndex,]
```

### SMOTE to oversample due to the skewness in target
Since we have already noted the severe imbalance in the values within the target variable, we used SMOTE function to created some arbirary data in minor class to make the whole categorical distribution in target variable balanced. 
The SMOTE function oversamples your rare event by using bootstrapping and k-nearest neighbor to synthetically create additional observations of that event. 

```{r}
#oversample train data to balance 
trainBalance <- SMOTE(Attrition ~ ., train, perc.over = 400, perc.under=131)
table(trainBalance$Attrition)
prop.table(table(trainBalance$Attrition))
```

We create some data in train set and now the proportion of "NO" "Yes" in target variable changed from 0.84:0.16 to 0.51:0.49. We are ready to build model based on this balanced data now. 

## Random Forest Classifier 
Since all interested in decidsion trees. We'll build an Random Forest Classifier to predict employees' attrition without lossing any variables. From Wikipedia, Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. We will first train such a model by using default tuning parametsrs from caret package. 

### Train model with default setting 

```{r}
set.seed(1234) #set seed to reproducible 
ctrl <- trainControl(method = "cv", number = 5)
rf_model <- train(Attrition~., data = trainBalance, method = "rf", trControl =ctrl)
print(rf_model)
plot(rf_model)
``` 

We chose the optimal model base on highest accuray, this model has mtry(Number of variables randomly sampled as candidates at each split) euqals to 2. 

Having fitted our forest of trees with our parameters to the training set against our target variable, we now have a learning model "rf_model" which we can make predictions out of. Our next step is to make predictions by using this model. 

```{r}
predictors <- names(trainBalance)[names(trainBalance) != 'Attrition']
pred_rf <- predict(rf_model$finalModel, test[,predictors],type = "prob")

#use AUC to measure the prediction 
auc <- roc(test$Attrition, pred_rf[,1])
print(auc)
plot(auc, ylim=c(0,1), print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
```

Accuracy of the model: 
We use AUC(area under the curve) to measure the prediction results. As observed, our Random Forest returns an AUC of 0.8435 for its predictions. 

### Tuning parameters
To further explore more, we tryied to tune the parameters in random forest model to see if we can find a better model. We focused on the below two parameters since they are most likely to have the biggest effect on your final accuracy.
mtry: Number of variables randomly sampled as candidates at each split.
ntree: Number of trees to grow.

Unfortunately, only mtry parameter is available in caret package for tuning. Normally the larger ntree parameter the better the model, but when the number is big enough it's effect decrease. We noticed that in caret packge the ntree parameter is 500. We guess that's a farily high number to make the model accurate.

In order to tune the mtry parameter, we found that there are two ways to tune the mtry parameter: 

Random Search: try random values within a range.

Grid Search: define a grid of algorithm parameters to try.

We performed grid search by setting mtry =c(1:23). The result based on training accuracy gives us the optimal model when mtry =3. So we use this tuned parameter to build our final model. 

```{r eval = TRUE}
#grid search to choose the best mtry
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(1234)
#since the ideal mtry is between sqrtq to 2/q.
tunegrid <- expand.grid(.mtry=c(1:23))
#tunegrid <- expand.grid(.mtry=8)
rf_gridsearch <- train(Attrition~., data=trainBalance, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
```

```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
set.seed(1234)
tunegrid <- expand.grid(.mtry= 3)
rf_optimal <- train(Attrition~., data=trainBalance, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
rf_optimal

predictors <- names(trainBalance)[names(trainBalance) != 'Attrition']
pred_rf_gridsearch <- predict(rf_optimal$finalModel,test[,predictors],type = "prob")
auc <- roc(test$Attrition, pred_rf_gridsearch[,1])
print(auc)
plot(auc, ylim=c(0,1),  print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
```

After tuning parameter we updated our model from using mtry =2 to mtry =3. As observed, our moel gives us a little better AUC. There is no huge difference between these two models. Our Guess is when we limited the number of mtry (number of the features randomly selected), we also limited the depth of the tree, so the result has smaller variance, since normally the deeper the forest, the smaller bias but bigger variance. 

Accuracy of the model: 

On first glance this might seem to be a very good performing model. From the AUC graph we can also find that the line quickly goes to 0.6 with high specificity, which means 60% class is easy to predict. Then the growth rate turned slow and hardly reached 1, meaning the last 20% class is hard to predict correctly. 

### Feature Ranking 
We used varImp function to discover which features within our dataset has been given most importance through the Random Forest algorithm. 

```{r}
#since the best mtry = 3
importance_grid <- varImp(rf_optimal,scale = FALSE)
print(importance_grid)
#plot(importance_grid)
ggplot(importance_grid)+ggtitle("Random Forest Feature Importance")
```

 Most RF important features: OverTime
It's reasonable that overTime affects people's satisfaction derived from any job. It seems our classifier has cought on to this. 
We're suprised that marital status are not in the first 20 importance. It may because marital status has high correlation with other features(such as Age). We can also reflect back to the correlations between features we made in Data Exploratory Analysis section.

## CONCLUSION
Fom some basic Exploratory Data Analysis to feature engineering to implementing two random forest classifier by tuning parameters, we have constructed a simple process on predicting employee attrition. And our final model has AUC 0.84, which is pretty good. 

In the future, there are still a lot to improve. Oversampling the minority class of target variables can be done by better strategy besides SMOTE. Because we created over 400 artifical points to balance the data, it might effect the accuracy of our model. Also, more features could be engineered from the data to make better performance. 

## Appendix -- Further discover 

We're wondering if building a random forest classifier only using the first 21 variables based on their importance that we discovred above, what's its performance comparing to that of our final model that included all variables. The motivation is to reduce HR's burden when evaluating an employee's attrition possibility. If this new model has similar performance than our final model(reducing the features will highly possibly reduce the performance, but a minor decrease is tolerable), we can use it instead. So an HR donesn't have to collect all 31 variables from employee but only those necessary 21.

To do this, we just need to use the data only containing those important 21 features. And perform the same process on this new data set to build the model. 

```{r}
# Select the subset of data only containing important 21 features 
dataSlct <- data[, c("Attrition", "OverTime", "StockOptionLevel","TotalWorkingYears",
"MonthlyIncome","JobInvolvement","Age","EnvironmentSatisfaction", "JobSatisfaction",
"YearsWithCurrManager","YearsAtCompany","JobLevel","YearsInCurrentRole",
"DailyRate","EmployeeNumber","TrainingTimesLastYear","DistanceFromHome","Education",
"NumCompaniesWorked","HourlyRate","WorkLifeBalance","MaritalStatus")]
```


```{r}
#create dummy variable
dummySlct <- model.matrix(Attrition ~ ., dataSlct)
new <- data.frame(dummySlct) # change back to a datafram 
new <- new[,-1] # get rid first useless columne
Attrition <- dataSlct$Attrition
dataDummy <- cbind(Attrition,new) #add Attrition back 
 
```

```{r}
#partition 
set.seed(1234)
splitIndex <- createDataPartition(dataDummy$Attrition, p =0.8, list =FALSE, times =1)
test <- dataDummy[-splitIndex,]
train <- dataDummy[splitIndex,]
```

```{r}
#oversampling 
trainBalance <- SMOTE(Attrition ~ ., train, perc.over = 400, perc.under=131)
table(trainBalance$Attrition)
prop.table(table(trainBalance$Attrition))
```

```{r}
#building modeling 
control <- trainControl(method="repeatedcv", number=10, repeats=3)
set.seed(1234)
tunegrid <- expand.grid(.mtry= 3)
rf_optimal <- train(Attrition~., data=trainBalance, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
rf_optimal
saveRDS(rf_optimal, file = "rf_optimal.rda")
```

```{r}
#analyzing performance
predictors <- names(trainBalance)[names(trainBalance) != 'Attrition']
length(predictors)
pred_rf_gridsearch <- predict(rf_optimal$finalModel,test[,predictors],type = "prob")
auc <- roc(test$Attrition, pred_rf_gridsearch[,1])
print(auc)
plot(auc, ylim=c(0,1),  print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
```

As we can see, the new model using only 21 features has AUC equals to 0.77. Comparing to our final model which has AUC equals to 0.83, this model decreases performance a lot. In this case, we'll suggest HR to consider all features if they use our model to do attrition prediction. Decreasing the number of feature and improving the performance of model are always a trad-off problem. But there are also many ways to improve, such as changing the number of selected features or using other features to build model, in order to reach a balance. 

##Conclusion: 

We were able to accomplish our two main objectives. Develop an accurate model for predicting employee attrition and identify possible areas of discrimination based on various characteristics. By utilizing various types of graph we were able to emphasize the trends by possible gender discrimination, and employee attrition. Though as we answered our objective questions, we began developing questions that could not be answered by the dataset such as how happy are IBM employees, was this data comparable to other technology companies, and also what incentive systems are in place to ensure accurate responses from employees and incentive to perform well.

The most prominent income gap between genders was presented in higher-level job roles such as Manager and Research director. One of these gaps represented a $18,168 difference in yearly salary, which should be alarming to the Human resource department.
Other characteristics of attrition we found were that based on job roles Healthcare representatives, manufacturing directors, and laboratory technicians, which represented the top three jobs employees leave. What we would recommend is reviewing the hiring process and criteria for these positions, understanding the responsibilities they hold, and whether their working environment emphasizes a long-term relationship with the company.
