---
title: "final analysis"
author: "Ria Pan"
date: "May 6, 2017"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pROC)
library(tidyverse)
library(caret)
library(DMwR)
library(randomForest)
library(e1071)
```

#Final Analysis

## Introduction
After our exploratory anlaysis, we got a sense of how correlated one feature is to the other and created some visualisations. Based on that result, we are going to build a predictive model on emplyee's attrition. To start off, we will first construct some feature engineering. 

```{r}
#can't do read_csv here, need be "dataframe" 
data <- read.csv('IBM-HR-Employee-Attrition.csv')
```

## Feature engineering & Categorical Encoding
Noticing that StandardHours, Over18, EmployeeCount has only 1 demension. We deleted these redundant features. 

```{r}
data<- data[-27]
data <- data[-22]
data <- data[-9]
```

Having identified which features contain categorical data in our dataset, we encoded those categorical variables into numerical values for further exploration. We used model.matrix method to creates encoded dummy variables from the categorical variables.

```{r}
dummy <- model.matrix(Attrition ~ ., data)
new <- data.frame(dummy) # change back to a datafram 
new <- new[,-1] # get rid first useless columne
Attrition <- data$Attrition
dataDummy <- cbind(Attrition,new) #add Attrition back 
head(dataDummy[,3:4])
```

However after quick inspection of the counts of the number of 'Yes' and 'No' in the target variable(Attrition), we found that there is quite a large skew in target as shown

```{r}
#check to find that the data is skewed 
ggplot(dataDummy) + geom_histogram(aes (x = Attrition),stat = "count")
prop.table(table(dataDummy$Attrition))
table(dataDummy$Attrition)
```

In this case we have to keep in mind that there is quite a big imbalance in our target variable. And with imbalanced data sets, an algorithm doesnâ€™t get the necessary information about the minority class to make an accurate prediction. In order to solve imbalanced data, we have considered many techniques such as repeated oversampling and undersampling. And we decided to use an oversampling technique known as SMOTE to treat this imbalance, which will be performed in next section. 


##Implementing Models
Having performed some exploratory data analysis and simple feature engineering as well as having ensured that all categorical values are encoded, we are now ready to proceed onto building our models.

###Splitting Data into Train and Test sets
Before we start training a model, we partitioned our dataset into a training set and a test set with proportion 8:2.

```{r}
#split data into train and test 
set.seed(1234)
splitIndex <- createDataPartition(dataDummy$Attrition, p =0.8, list =FALSE, times =1)
test <- dataDummy[-splitIndex,]
train <- dataDummy[splitIndex,]
```

### SMOTE to oversample due to the skewness in target
Since we have already noted the severe imbalance in the values within the target variable, we used SMOTE function to created some arbirary data in minor class to make the whole categorical distribution in target variable balanced. 
The SMOTE function oversamples your rare event by using bootstrapping and k-nearest neighbor to synthetically create additional observations of that event. 

```{r}
#oversample train data to balance 
trainBalance <- SMOTE(Attrition ~ ., train, perc.over = 400, perc.under=131)
table(trainBalance$Attrition)
prop.table(table(trainBalance$Attrition))
```

We create some data in train set and now the proportion of "NO" "Yes" in target variable changed from 0.84:0.16 to 0.51:0.49. We are ready to build model based on this balanced data now. 

## Random Forest Classifier 
Since all interested in decidsion trees. We'll build an Random Forest Classifier to predict employees' attrition without lossing any variables. From Wikipedia, Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. We will first train such a model by using default tuning parametsrs from caret package. 

### Train model with default setting 

```{r}
set.seed(1234) #set seed to reproducible 
ctrl <- trainControl(method = "cv", number = 5)
rf_model <- train(Attrition~., data = trainBalance, method = "rf", trControl =ctrl)
print(rf_model)
plot(rf_model)
``` 

We chose the optimal model base on highest accuray, this model has mtry(Number of variables randomly sampled as candidates at each split) euqals to 2. 

Having fitted our forest of trees with our parameters to the training set against our target variable, we now have a learning model "rf_model" which we can make predictions out of. Our next step is to make predictions by using this model. 

```{r}
predictors <- names(trainBalance)[names(trainBalance) != 'Attrition']
pred_rf <- predict(rf_model$finalModel, test[,predictors],type = "prob")

#use AUC to measure the prediction 
auc <- roc(test$Attrition, pred_rf[,1])
print(auc)
plot(auc, ylim=c(0,1), print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
```

Accuracy of the model: 
We use AUC(area under the curve) to measure the prediction results. As observed, our Random Forest returns an AUC of 0.8435 for its predictions. 

### Tuning parameters
To further explore more, we tryied to tune the parameters in random forest model to see if we can find a better model. We focused on the below two parameters since they are most likely to have the biggest effect on your final accuracy.
mtry: Number of variables randomly sampled as candidates at each split.
ntree: Number of trees to grow.

Unfortunately, only mtry parameter is available in caret package for tuning. Normally the larger ntree parameter the better the model, but when the number is big enough it's effect decrease. We noticed that in caret packge the ntree parameter is 500. We guess that's a farily high number to make the model accurate.

In order to tune the mtry parameter, we found that there are two ways to tune the mtry parameter: 

Random Search: try random values within a range.

Grid Search: define a grid of algorithm parameters to try.

We performed grid search by setting mtry =c(1:23). The result based on training accuracy gives us the optimal model when mtry =3. So we use this tuned parameter to build our final model. 

```{r eval = FALSE}
#grid search to choose the best mtry
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(1234)
#since the ideal mtry is between sqrtq to 2/q.
tunegrid <- expand.grid(.mtry=c(1:23))
#tunegrid <- expand.grid(.mtry=8)
rf_gridsearch <- train(Attrition~., data=trainBalance, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
```

```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
set.seed(1234)
tunegrid <- expand.grid(.mtry= 3)
rf_optimal <- train(Attrition~., data=trainBalance, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
rf_optimal

predictors <- names(trainBalance)[names(trainBalance) != 'Attrition']
pred_rf_gridsearch <- predict(rf_optimal$finalModel,test[,predictors],type = "prob")
auc <- roc(test$Attrition, pred_rf_gridsearch[,1])
print(auc)
plot(auc, ylim=c(0,1),  print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
```

After tuning parameter we updated our model from using mtry =2 to mtry =3. As observed, our moel gives us a little better AUC. There is no huge difference between these two models. Our Guess is when we limited the number of mtry (number of the features randomly selected), we also limited the depth of the tree, so the result has smaller variance, since normally the deeper the forest, the smaller bias but bigger variance. 

Accuracy of the model: 

On first glance this might seem to be a very good performing model. From the AUC graph we can also find that the line quickly goes to 0.6 with high specificity, which means 60% class is easy to predict. Then the growth rate turned slow and hardly reached 1, meaning the last 20% class is hard to predict correctly. 

### Feature Ranking 
We used varImp function to discover which features within our dataset has been given most importance through the Random Forest algorithm. 

```{r}
#since the best mtry = 3
importance_grid <- varImp(rf_optimal,scale = FALSE)
print(importance_grid)
#plot(importance_grid)
ggplot(importance_grid)+ggtitle("Random Forest Feature Importance")
```

 Most RF important features: OverTime
It's reasonable that overTime affects people's satisfaction derived from any job. It seems our classifier has cought on to this. 
We're suprised that marital status are not in the first 20 importance. It may because marital status has high correlation with other features(such as Age). We can also reflect back to the correlations between features we made in Data Exploratory Analysis section.

## CONCLUSION
Fom some basic Exploratory Data Analysis to feature engineering to implementing two random forest classifier by tuning parameters, we have constructed a simple process on predicting employee attrition. And our final model has AUC 0.84, which is pretty good. 

In the future, there are still a lot to improve. Oversampling the minority class of target variables can be done by better strategy besides SMOTE. Because we created over 400 artifical points to balance the data, it might effect the accuracy of our model. Also, more features could be engineered from the data to make better performance. 

## Appendix -- Further discover 

We're wondering if building a random forest classifier only using the first 21 variables based on their importance that we discovred above, what's its performance comparing to that of our final model that included all variables. The motivation is to reduce HR's burden when evaluating an employee's attrition possibility. If this new model has similar performance than our final model(reducing the features will highly possibly reduce the performance, but a minor decrease is tolerable), we can use it instead. So an HR donesn't have to collect all 31 variables from employee but only those necessary 21.

To do this, we just need to use the data only containing those important 21 features. And perform the same process on this new data set to build the model. 

```{r}
# Select the subset of data only containing important 21 features 
dataSlct <- data[, c("Attrition", "OverTime", "StockOptionLevel","TotalWorkingYears",
"MonthlyIncome","JobInvolvement","Age","EnvironmentSatisfaction", "JobSatisfaction",
"YearsWithCurrManager","YearsAtCompany","JobLevel","YearsInCurrentRole",
"DailyRate","EmployeeNumber","TrainingTimesLastYear","DistanceFromHome","Education",
"NumCompaniesWorked","HourlyRate","WorkLifeBalance","MaritalStatus")]
```


```{r}
#create dummy variable
dummySlct <- model.matrix(Attrition ~ ., dataSlct)
new <- data.frame(dummySlct) # change back to a datafram 
new <- new[,-1] # get rid first useless columne
Attrition <- dataSlct$Attrition
dataDummy <- cbind(Attrition,new) #add Attrition back 
 
```

```{r}
#partition 
set.seed(1234)
splitIndex <- createDataPartition(dataDummy$Attrition, p =0.8, list =FALSE, times =1)
test <- dataDummy[-splitIndex,]
train <- dataDummy[splitIndex,]
```

```{r}
#oversampling 
trainBalance <- SMOTE(Attrition ~ ., train, perc.over = 400, perc.under=131)
table(trainBalance$Attrition)
prop.table(table(trainBalance$Attrition))
```

```{r}
#building modeling 
control <- trainControl(method="repeatedcv", number=10, repeats=3)
set.seed(1234)
tunegrid <- expand.grid(.mtry= 3)
rf_optimal <- train(Attrition~., data=trainBalance, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
rf_optimal
```

```{r}
#analyzing performance
predictors <- names(trainBalance)[names(trainBalance) != 'Attrition']
length(predictors)
pred_rf_gridsearch <- predict(rf_optimal$finalModel,test[,predictors],type = "prob")
auc <- roc(test$Attrition, pred_rf_gridsearch[,1])
print(auc)
plot(auc, ylim=c(0,1),  print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
```

As we can see, the new model using only 21 features has AUC equals to 0.77. Comparing to our final model which has AUC equals to 0.83, this model decreases performance a lot. In this case, we'll suggest HR to consider all features if they use our model to do attrition prediction. Decreasing the number of feature and improving the performance of model are always a trad-off problem. But there are also many ways to improve, such as changing the number of selected features or using other features to build model, in order to reach a balance. 